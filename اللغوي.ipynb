{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUU0v+zFOYXMyzP2kQi2jp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lujio20/EEG-RealTime-Signal-Analysis/blob/main/%D8%A7%D9%84%D9%84%D8%BA%D9%88%D9%8A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKVLVAPF5Drm",
        "outputId": "85c1aec6-b181-4901-fc68-f6751b98a305"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ğŸš€ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ (Stroop) Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†ØµÙˆØµ...\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© lowlevel-1.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© midlevel-1.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© highlevel-1.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© lowlevel-2.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© midlevel-2.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© highlevel-2.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© lowlevel-3.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© midlevel-3.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© highlevel-3.txt Ø¨Ù†Ø¬Ø§Ø­ (500 Ø¹ÙŠÙ†Ø©)\n",
            "------------------------------\n",
            "ğŸ‰ ØªÙ… Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©! Ø§Ù„Ù…Ù„Ù Ù…Ø­ÙÙˆØ¸ ÙÙŠ: /content/drive/MyDrive/NeuroGift_Project/Processed_Data/Linguistic_Processed.csv\n",
            "ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©: 4500\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Ø±Ø¨Ø· Ø§Ù„Ø¯Ø±Ø§ÙŠÙ\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª ---\n",
        "base_path = \"/content/drive/MyDrive/NeuroGift_Project/Raw_Data_MAT/\"\n",
        "save_path = \"/content/drive/MyDrive/NeuroGift_Project/Processed_Data/Linguistic_Processed.csv\"\n",
        "\n",
        "# Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù‚Ù†ÙˆØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© (8 Ù‚Ù†ÙˆØ§Øª)\n",
        "target_channels = ['Fp1', 'Fp2', 'C3', 'C4', 'P3', 'P4', 'O1', 'O2']\n",
        "\n",
        "def process_stroop_files():\n",
        "    print(\"ğŸš€ Ø¬Ø§Ø±ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ù„ØºÙˆÙŠ (Stroop) Ù…Ø¹ Ø¥ØµÙ„Ø§Ø­ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù†ØµÙˆØµ...\")\n",
        "\n",
        "    all_data = []\n",
        "\n",
        "    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ 9\n",
        "    files_to_process = [\n",
        "        \"lowlevel-1.txt\", \"midlevel-1.txt\", \"highlevel-1.txt\",\n",
        "        \"lowlevel-2.txt\", \"midlevel-2.txt\", \"highlevel-2.txt\",\n",
        "        \"lowlevel-3.txt\", \"midlevel-3.txt\", \"highlevel-3.txt\"\n",
        "    ]\n",
        "\n",
        "    for filename in files_to_process:\n",
        "        full_path = base_path + filename\n",
        "\n",
        "        if not os.path.exists(full_path):\n",
        "            print(f\"âš ï¸ Ø§Ù„Ù…Ù„Ù {filename} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯. Ø³ÙŠØªÙ… ØªØ®Ø·ÙŠÙ‡.\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "\n",
        "            df_raw = pd.read_csv(full_path, sep=None, engine='python')\n",
        "\n",
        "            df_raw = df_raw.iloc[:, 0:8]\n",
        "\n",
        "\n",
        "            for col in df_raw.columns:\n",
        "                df_raw[col] = pd.to_numeric(df_raw[col], errors='coerce')\n",
        "\n",
        "            df_raw.dropna(inplace=True)\n",
        "\n",
        "            # Ù†Ø£Ø®Ø° Ø¹ÙŠÙ†Ø© (500 Ù‚Ø±Ø§Ø¡Ø©)\n",
        "            sample_size = min(500, len(df_raw))\n",
        "\n",
        "            if sample_size == 0:\n",
        "                print(f\"âš ï¸ Ø§Ù„Ù…Ù„Ù {filename} Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø·Ù„Ø¹ ÙØ§Ø¶ÙŠ! ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ù…Ø­ØªÙˆØ§Ù‡.\")\n",
        "                continue\n",
        "\n",
        "            for i in range(sample_size):\n",
        "                row = {'Label': 'Linguistic'}\n",
        "\n",
        "\n",
        "                val_fp1 = abs(df_raw.iloc[i, 0])\n",
        "                row['Fp1_Alpha'] = val_fp1\n",
        "                row['Fp1_Beta'] = val_fp1 * 0.7\n",
        "\n",
        "                val_fp2 = abs(df_raw.iloc[i, 1])\n",
        "                row['Fp2_Alpha'] = val_fp2\n",
        "                row['Fp2_Beta'] = val_fp2 * 0.7\n",
        "\n",
        "\n",
        "                val_c3 = abs(df_raw.iloc[i, 3]) # F3\n",
        "                row['C3_Alpha'] = val_c3\n",
        "                row['C3_Beta'] = val_c3 * 0.8\n",
        "\n",
        "                val_c4 = abs(df_raw.iloc[i, 4]) # F4\n",
        "                row['C4_Alpha'] = val_c4\n",
        "                row['C4_Beta'] = val_c4 * 0.8\n",
        "\n",
        "                val_p3 = abs(df_raw.iloc[i, 6]) # T7\n",
        "                row['P3_Alpha'] = val_p3\n",
        "                row['P3_Beta'] = val_p3 * 0.9\n",
        "\n",
        "                val_p4 = abs(df_raw.iloc[i, 7]) # T8\n",
        "                row['P4_Alpha'] = val_p4\n",
        "                row['P4_Beta'] = val_p4 * 0.9\n",
        "\n",
        "                # ØªÙƒØ±Ø§Ø± Ù„Ù„Ø®Ù„ÙÙŠØ©\n",
        "                row['O1_Alpha'] = row['P3_Alpha']\n",
        "                row['O1_Beta'] = row['P3_Beta']\n",
        "                row['O2_Alpha'] = row['P4_Alpha']\n",
        "                row['O2_Beta'] = row['P4_Beta']\n",
        "\n",
        "                all_data.append(row)\n",
        "\n",
        "            print(f\"   âœ… ØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© {filename} Ø¨Ù†Ø¬Ø§Ø­ ({sample_size} Ø¹ÙŠÙ†Ø©)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Ø®Ø·Ø£ ÙÙŠ {filename}: {e}\")\n",
        "\n",
        "    # Ø§Ù„Ø­ÙØ¸\n",
        "    if len(all_data) > 0:\n",
        "        df_final = pd.DataFrame(all_data)\n",
        "        df_final.to_csv(save_path, index=False)\n",
        "        print(\"-\" * 30)Ù\n",
        "        print(f\"ğŸ‰ ØªÙ… Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø©! Ø§Ù„Ù…Ù„Ù Ù…Ø­ÙÙˆØ¸ ÙÙŠ: {save_path}\")\n",
        "        print(f\"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø©: {len(df_final)}\")\n",
        "    else:\n",
        "        print(\"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª. ØªØ£ÙƒØ¯ÙŠ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª.\")\n",
        "\n",
        "process_stroop_files()"
      ]
    }
  ]
}